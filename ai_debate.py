"""
AI Debate - AIåŒå£«ã®ãƒ‡ã‚£ãƒ™ãƒ¼ãƒˆã‚’è¦³æˆ¦ã™ã‚‹ã‚¢ãƒ—ãƒª
2ã¤ã®AIãŒè³›æˆæ´¾ã¨åå¯¾æ´¾ã«åˆ†ã‹ã‚Œã¦è­°è«–ã‚’ç¶šã‘ã¾ã™
"""

import time
import sys
import subprocess
import threading
from concurrent.futures import ThreadPoolExecutor, Future


class RateLimitError(Exception):
    """APIåˆ¶é™ã‚¨ãƒ©ãƒ¼"""
    pass


# ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®é¸æŠï¼ˆollama ã¾ãŸã¯ geminiï¼‰
BACKEND = "groq"  # "ollama", "gemini", or "groq"

# Ollamaè¨­å®š
OLLAMA_MODEL = "llama3.2"  # ã¾ãŸã¯ gemma2, mistral ãªã©

# Geminiè¨­å®šï¼ˆç„¡æ–™æ : 15ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/åˆ†ï¼‰
GEMINI_API_KEY = "AIzaSyB7WOCczBjpmBt0dtt44zu_dFDMruUfpuI"

# Groqè¨­å®šï¼ˆç„¡æ–™æ : 30ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/åˆ†ã€14400ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/æ—¥ï¼‰
GROQ_API_KEY = os.getenv("GROQ_API_KEY", "")  # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—


def get_llm_response(prompt: str, system_prompt: str) -> str:
    """LLMã‹ã‚‰ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—"""
    if BACKEND == "ollama":
        return get_ollama_response(prompt, system_prompt)
    elif BACKEND == "gemini":
        return get_gemini_response(prompt, system_prompt)
    elif BACKEND == "groq":
        return get_groq_response(prompt, system_prompt)
    else:
        raise ValueError(f"Unknown backend: {BACKEND}")


def get_ollama_response(prompt: str, system_prompt: str) -> str:
    """Ollamaã‚’ä½¿ç”¨ã—ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—"""
    import requests

    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": OLLAMA_MODEL,
            "prompt": prompt,
            "system": system_prompt,
            "stream": False,
        },
    )
    response.raise_for_status()
    return response.json()["response"]


def get_gemini_response(prompt: str, system_prompt: str) -> str:
    """Gemini APIã‚’ä½¿ç”¨ã—ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—ï¼ˆæ–°ã—ã„google-genaiãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ï¼‰"""
    from google import genai
    import os

    api_key = GEMINI_API_KEY or os.environ.get("GEMINI_API_KEY")
    if not api_key:
        raise ValueError("GEMINI_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„")

    client = genai.Client(api_key=api_key)
    full_prompt = f"{system_prompt}\n\n{prompt}"

    max_retries = 5
    for attempt in range(max_retries):
        try:
            response = client.models.generate_content(
                model="gemma-3-12b-it",
                contents=full_prompt
            )
            return response.text
        except Exception as e:
            error_msg = str(e).lower()
            if "quota" in error_msg or "rate" in error_msg or "resource" in error_msg or "exhausted" in error_msg or "429" in error_msg:
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®å ´åˆã¯å¾…ã£ã¦ãƒªãƒˆãƒ©ã‚¤
                wait_time = 10 * (attempt + 1)  # 10ç§’ã€20ç§’ã€30ç§’...
                if attempt < max_retries - 1:
                    print_colored(f" [ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã€{wait_time}ç§’å¾…æ©Ÿä¸­...]", "yellow", end="")
                    time.sleep(wait_time)
                    continue
                else:
                    raise RateLimitError("APIåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚")
            raise


def get_groq_response(prompt: str, system_prompt: str) -> str:
    """Groq APIã‚’ä½¿ç”¨ã—ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—ï¼ˆè¶…é«˜é€Ÿï¼‰"""
    from groq import Groq
    import os

    api_key = GROQ_API_KEY or os.environ.get("GROQ_API_KEY")
    if not api_key:
        raise ValueError("GROQ_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„")

    client = Groq(api_key=api_key)

    max_retries = 3
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=200,
            )
            return response.choices[0].message.content
        except Exception as e:
            error_msg = str(e).lower()
            if "rate" in error_msg or "limit" in error_msg or "429" in error_msg:
                wait_time = 5 * (attempt + 1)
                if attempt < max_retries - 1:
                    print_colored(f" [ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã€{wait_time}ç§’å¾…æ©Ÿä¸­...]", "yellow", end="")
                    time.sleep(wait_time)
                    continue
                else:
                    raise RateLimitError("APIåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚")
            raise


def create_debater_prompt(role: str, topic: str, personality: dict) -> str:
    """ãƒ‡ã‚£ãƒ™ãƒ¼ã‚¿ãƒ¼ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ"""
    stance = "è³›æˆ" if role == "pro" else "åå¯¾"

    return f"""ã‚ãªãŸã¯{personality['name']}ã¨ã„ã†åå‰ã®{personality['age']}ã®{personality['job']}ã§ã™ã€‚
ã€Œ{topic}ã€ã«{stance}ã®ç«‹å ´ã§è­°è«–ã—ã¦ã„ã¾ã™ã€‚

ã€æœ€é‡è¦ãƒ«ãƒ¼ãƒ«ã€‘
- å¿…ãšã€Œ{topic}ã€ã®å…·ä½“çš„ãªå†…å®¹ã«ã¤ã„ã¦è©±ã™ã“ã¨
- å…·ä½“ä¾‹ã‚„æ ¹æ‹ ã‚’æŒ™ã’ã¦è­°è«–ã™ã‚‹ã“ã¨
- ç›¸æ‰‹ã®äººæ ¼ã§ã¯ãªãã€ç›¸æ‰‹ã®ã€Œä¸»å¼µã®å†…å®¹ã€ã«åè«–ã™ã‚‹ã“ã¨

ã€è©±ã—æ–¹ã®ãƒ«ãƒ¼ãƒ«ã€‘
- ä¸€æ–‡ã€œäºŒæ–‡ã§è¿”ç­”ã™ã‚‹
- {personality['tone']}ã§è©±ã™
- ã€Œã€œã§ã™ã€ã€Œã€œã¾ã™ã€ã®èª¬æ˜å£èª¿ã¯ç¦æ­¢

ã€ç¦æ­¢äº‹é …ã€‘
- ç›¸æ‰‹ã‚’ã€ŒåŒ–çŸ³ã€ã€Œé ­ãŒå›ºã„ã€ãªã©äººæ ¼æ”»æ’ƒã™ã‚‹ã“ã¨
- æŠ½è±¡çš„ãªè¨€ã„åˆã„ã«çµ‚å§‹ã™ã‚‹ã“ã¨
- ãƒ†ãƒ¼ãƒã‹ã‚‰è„±ç·šã™ã‚‹ã“ã¨

æ€§æ ¼: {personality['personality']}"""


# éŸ³å£°è¨­å®š
VOICE_ENABLED = True  # éŸ³å£°ã‚’ã‚ªãƒ•ã«ã—ãŸã„å ´åˆã¯ False

# ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®š
CHARACTERS = {
    "pro": {
        "name": "ã‚¿ã‚±ã‚·",
        "age": "28æ­³",
        "job": "ITä¼æ¥­å‹¤å‹™",
        "tone": "ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªã‚¿ãƒ¡å£",
        "personality": "ç†±ããªã‚Šã‚„ã™ã„ã€è² ã‘ãšå«Œã„ã€ãŸã¾ã«çš®è‚‰ã‚’è¨€ã†",
        "voice": "Rocko (æ—¥æœ¬èªï¼ˆæ—¥æœ¬ï¼‰)",  # ç”·æ€§å£°
    },
    "con": {
        "name": "ãƒ¦ãƒŸ",
        "age": "32æ­³",
        "job": "å‡ºç‰ˆç¤¾ç·¨é›†è€…",
        "tone": "å°‘ã—è¾›è¾£ã ã‘ã©çŸ¥çš„ãªå£èª¿",
        "personality": "å†·é™ã€è«–ç†çš„ã€ç›¸æ‰‹ã®çŸ›ç›¾ã‚’çªãã®ãŒå¾—æ„",
        "voice": "Kyoko",  # å¥³æ€§å£°
    }
}


def speak(text: str, voice: str):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’éŸ³å£°ã§èª­ã¿ä¸Šã’ã‚‹ï¼ˆãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ï¼‰"""
    if not VOICE_ENABLED:
        return
    subprocess.run(["say", "-v", voice, "-r", "140", text], check=True)


def speak_async(text: str, voice: str) -> threading.Thread:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’éŸ³å£°ã§èª­ã¿ä¸Šã’ã‚‹ï¼ˆéåŒæœŸï¼‰"""
    if not VOICE_ENABLED:
        return None
    thread = threading.Thread(target=speak, args=(text, voice))
    thread.start()
    return thread


def print_colored(text: str, color: str, end: str = "\n"):
    """è‰²ä»˜ããƒ†ã‚­ã‚¹ãƒˆã‚’å‡ºåŠ›"""
    colors = {
        "red": "\033[91m",
        "green": "\033[92m",
        "yellow": "\033[93m",
        "blue": "\033[94m",
        "magenta": "\033[95m",
        "cyan": "\033[96m",
        "reset": "\033[0m",
    }
    print(f"{colors.get(color, '')}{text}{colors['reset']}", end=end)


def print_header():
    """ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¡¨ç¤º"""
    print("\n" + "=" * 60)
    print_colored("   AI DEBATE - AIåŒå£«ã®ãƒ‡ã‚£ãƒ™ãƒ¼ãƒˆã‚’è¦³æˆ¦ã—ã‚ˆã†", "cyan")
    print("=" * 60 + "\n")


def run_debate():
    """ãƒ‡ã‚£ãƒ™ãƒ¼ãƒˆã‚’å®Ÿè¡Œ"""
    print_header()

    # è­°é¡Œã‚’å…¥åŠ›
    print_colored("è­°é¡Œã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹: AIã¯äººé–“ã®ä»•äº‹ã‚’å¥ªã†ï¼‰", "yellow")
    topic = input("> ").strip()
    if not topic:
        topic = "AIã¯äººé–“ã®ä»•äº‹ã‚’å¥ªã†"

    print(f"\nè­°é¡Œ: ã€Œ{topic}ã€\n")
    print_colored("è³›æˆæ´¾ ğŸŸ¢ vs åå¯¾æ´¾ ğŸ”´", "magenta")
    print("-" * 60)
    print("Ctrl+C ã§çµ‚äº†\n")

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
    pro_system = create_debater_prompt("pro", topic, CHARACTERS["pro"])
    con_system = create_debater_prompt("con", topic, CHARACTERS["con"])

    print(f"\n{CHARACTERS['pro']['name']}ï¼ˆè³›æˆæ´¾ï¼‰vs {CHARACTERS['con']['name']}ï¼ˆåå¯¾æ´¾ï¼‰\n")

    # ä¼šè©±å±¥æ­´
    history = []

    # æœ€åˆã®ç™ºè¨€
    initial_prompt = f"ã€Œ{topic}ã€ã«ã¤ã„ã¦ã€å…·ä½“ä¾‹ã‚’ä¸€ã¤æŒ™ã’ã¦è‡ªåˆ†ã®æ„è¦‹ã‚’è¨€ã£ã¦ã€‚"

    turn = 0
    api_calls = 0
    max_calls_warning = 1400  # 1500å›/æ—¥ã®åˆ¶é™ã«è¿‘ã¥ã„ãŸã‚‰è­¦å‘Š

    pro_name = CHARACTERS["pro"]["name"]
    con_name = CHARACTERS["con"]["name"]

    try:
        while True:
            turn += 1

            # === è³›æˆæ´¾ã®ç™ºè¨€ ===
            print_colored(f"{pro_name}: ", "green", end="")
            sys.stdout.flush()

            if turn == 1:
                pro_prompt = initial_prompt
            else:
                pro_prompt = f"{con_name}ã€Œ{history[-1]}ã€\n\nâ†‘ã“ã®ä¸»å¼µã«å¯¾ã—ã¦å…·ä½“ä¾‹ã‚„æ ¹æ‹ ã‚’æŒ™ã’ã¦åè«–ã—ã¦ã€‚"

            pro_response = get_llm_response(pro_prompt, pro_system)
            api_calls += 1
            pro_text = pro_response.strip()
            print(pro_text)
            history.append(pro_text)

            # éŸ³å£°èª­ã¿ä¸Šã’
            speak(pro_text, CHARACTERS["pro"]["voice"])

            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼šæ¬¡ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¾ã§å¾…æ©Ÿ
            time.sleep(5)

            # === åå¯¾æ´¾ã®ç™ºè¨€ ===
            print_colored(f"{con_name}: ", "red", end="")
            sys.stdout.flush()

            con_prompt = f"{pro_name}ã€Œ{pro_text}ã€\n\nâ†‘ã“ã®ä¸»å¼µã«å¯¾ã—ã¦å…·ä½“ä¾‹ã‚„æ ¹æ‹ ã‚’æŒ™ã’ã¦åè«–ã—ã¦ã€‚"
            con_response = get_llm_response(con_prompt, con_system)
            api_calls += 1
            con_text = con_response.strip()
            print(con_text)
            history.append(con_text)

            # éŸ³å£°èª­ã¿ä¸Šã’
            speak(con_text, CHARACTERS["con"]["voice"])

            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼šæ¬¡ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¾ã§å¾…æ©Ÿ
            time.sleep(5)

    except KeyboardInterrupt:
        print("")  # æ”¹è¡Œ
    except RateLimitError as e:
        print_colored(f"\n\nâš ï¸ {e}", "yellow")
        print_colored("1åˆ†å¾…ã¤ã‹ã€æ˜æ—¥(æ—¥æœ¬æ™‚é–“9æ™‚ãƒªã‚»ãƒƒãƒˆ)ã«å†é–‹ã—ã¦ãã ã•ã„ã€‚", "yellow")

    print("\n" + "=" * 60)
    print_colored("ãƒ‡ã‚£ãƒ™ãƒ¼ãƒˆçµ‚äº†ï¼", "cyan")
    print(f"ãƒ©ã‚¦ãƒ³ãƒ‰æ•°: {turn}  APIä½¿ç”¨: {api_calls}å›")
    print("=" * 60 + "\n")


if __name__ == "__main__":
    run_debate()
